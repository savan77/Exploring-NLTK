{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP is a field of computer science and linguistic. It's an intersection between computer and human languages. Common task is natural language understanding. For more about Natural Language Processing visit <a href=\"https://en.wikipedia.org/wiki/Natural_language_processing\"> NLP</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can not use whole corpus as an input for parsing or text analysis. We have to break it down into something called tokens. Tokenization is a process of breaking stream of text into words, phrases or other meaningful elements. Most of the time tokenization is the first process in text analysis or NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK ( Natural Language Tool Kit for Python ) provides two types of functions for tokenization- sentence tokenization and word tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence Tokenization - We break down the stream of text into a set of sentences. In this case our tokens would be sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Tokenization - We break down the stream of text into a set of words. In this case our tokens would be words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import our dependency -nltk\n",
    "import nltk\n",
    "\n",
    "#import required modules\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "#uncomment below line if you haven't downloaded all nltk libs and corpus\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get text\n",
    "corpus = open('./data/MyText.txt','r').read()\n",
    "\n",
    "#here, f is a string or stream of text. Let's check it\n",
    "type(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have our text, let's perform tokenization on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"A ``knowledge engineer'' interviews experts in a certain domain and tries to embody their knowledge in a computer program for carrying out some task.\",\n",
       " 'How well this works depends on whether the intellectual mechanisms required for the task are within the present state of AI.',\n",
       " 'When this turned out not to be so, there were many disappointing results.',\n",
       " 'One of the first expert systems was MYCIN in 1974, which diagnosed bacterial infections of the blood and suggested treatments.',\n",
       " 'It did better than medical students or practicing doctors, provided its limitations were observed.',\n",
       " 'Namely, its ontology included bacteria, symptoms, and treatments and did not include patients, doctors, hospitals, death, recovery, and events occurring in time.',\n",
       " 'Its interactions depended on a single patient being considered.',\n",
       " 'Since the experts consulted by the knowledge engineers knew about patients, doctors, death, recovery, etc., it is clear that the knowledge engineers forced what the experts told them into a predetermined framework.',\n",
       " 'In the present state of AI, this has to be true.',\n",
       " 'The usefulness of current expert systems depends on their users having common sense.\\n']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sentence tokenization\n",
    "sentences = sent_tokenize(corpus)\n",
    "sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " '``',\n",
       " 'knowledge',\n",
       " 'engineer',\n",
       " \"''\",\n",
       " 'interviews',\n",
       " 'experts',\n",
       " 'in',\n",
       " 'a',\n",
       " 'certain',\n",
       " 'domain',\n",
       " 'and',\n",
       " 'tries',\n",
       " 'to',\n",
       " 'embody',\n",
       " 'their',\n",
       " 'knowledge',\n",
       " 'in',\n",
       " 'a',\n",
       " 'computer',\n",
       " 'program',\n",
       " 'for',\n",
       " 'carrying',\n",
       " 'out',\n",
       " 'some',\n",
       " 'task',\n",
       " '.',\n",
       " 'How',\n",
       " 'well',\n",
       " 'this',\n",
       " 'works',\n",
       " 'depends',\n",
       " 'on',\n",
       " 'whether',\n",
       " 'the',\n",
       " 'intellectual',\n",
       " 'mechanisms',\n",
       " 'required',\n",
       " 'for',\n",
       " 'the',\n",
       " 'task',\n",
       " 'are',\n",
       " 'within',\n",
       " 'the',\n",
       " 'present',\n",
       " 'state',\n",
       " 'of',\n",
       " 'AI',\n",
       " '.',\n",
       " 'When',\n",
       " 'this',\n",
       " 'turned',\n",
       " 'out',\n",
       " 'not',\n",
       " 'to',\n",
       " 'be',\n",
       " 'so',\n",
       " ',',\n",
       " 'there',\n",
       " 'were',\n",
       " 'many',\n",
       " 'disappointing',\n",
       " 'results',\n",
       " '.',\n",
       " 'One',\n",
       " 'of',\n",
       " 'the',\n",
       " 'first',\n",
       " 'expert',\n",
       " 'systems',\n",
       " 'was',\n",
       " 'MYCIN',\n",
       " 'in',\n",
       " '1974',\n",
       " ',',\n",
       " 'which',\n",
       " 'diagnosed',\n",
       " 'bacterial',\n",
       " 'infections',\n",
       " 'of',\n",
       " 'the',\n",
       " 'blood',\n",
       " 'and',\n",
       " 'suggested',\n",
       " 'treatments',\n",
       " '.',\n",
       " 'It',\n",
       " 'did',\n",
       " 'better',\n",
       " 'than',\n",
       " 'medical',\n",
       " 'students',\n",
       " 'or',\n",
       " 'practicing',\n",
       " 'doctors',\n",
       " ',',\n",
       " 'provided',\n",
       " 'its',\n",
       " 'limitations',\n",
       " 'were',\n",
       " 'observed',\n",
       " '.',\n",
       " 'Namely',\n",
       " ',',\n",
       " 'its',\n",
       " 'ontology',\n",
       " 'included',\n",
       " 'bacteria',\n",
       " ',',\n",
       " 'symptoms',\n",
       " ',',\n",
       " 'and',\n",
       " 'treatments',\n",
       " 'and',\n",
       " 'did',\n",
       " 'not',\n",
       " 'include',\n",
       " 'patients',\n",
       " ',',\n",
       " 'doctors',\n",
       " ',',\n",
       " 'hospitals',\n",
       " ',',\n",
       " 'death',\n",
       " ',',\n",
       " 'recovery',\n",
       " ',',\n",
       " 'and',\n",
       " 'events',\n",
       " 'occurring',\n",
       " 'in',\n",
       " 'time',\n",
       " '.',\n",
       " 'Its',\n",
       " 'interactions',\n",
       " 'depended',\n",
       " 'on',\n",
       " 'a',\n",
       " 'single',\n",
       " 'patient',\n",
       " 'being',\n",
       " 'considered',\n",
       " '.',\n",
       " 'Since',\n",
       " 'the',\n",
       " 'experts',\n",
       " 'consulted',\n",
       " 'by',\n",
       " 'the',\n",
       " 'knowledge',\n",
       " 'engineers',\n",
       " 'knew',\n",
       " 'about',\n",
       " 'patients',\n",
       " ',',\n",
       " 'doctors',\n",
       " ',',\n",
       " 'death',\n",
       " ',',\n",
       " 'recovery',\n",
       " ',',\n",
       " 'etc.',\n",
       " ',',\n",
       " 'it',\n",
       " 'is',\n",
       " 'clear',\n",
       " 'that',\n",
       " 'the',\n",
       " 'knowledge',\n",
       " 'engineers',\n",
       " 'forced',\n",
       " 'what',\n",
       " 'the',\n",
       " 'experts',\n",
       " 'told',\n",
       " 'them',\n",
       " 'into',\n",
       " 'a',\n",
       " 'predetermined',\n",
       " 'framework',\n",
       " '.',\n",
       " 'In',\n",
       " 'the',\n",
       " 'present',\n",
       " 'state',\n",
       " 'of',\n",
       " 'AI',\n",
       " ',',\n",
       " 'this',\n",
       " 'has',\n",
       " 'to',\n",
       " 'be',\n",
       " 'true',\n",
       " '.',\n",
       " 'The',\n",
       " 'usefulness',\n",
       " 'of',\n",
       " 'current',\n",
       " 'expert',\n",
       " 'systems',\n",
       " 'depends',\n",
       " 'on',\n",
       " 'their',\n",
       " 'users',\n",
       " 'having',\n",
       " 'common',\n",
       " 'sense',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word tokenization\n",
    "words = word_tokenize(corpus)\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words are most common words such as I, my, for etc.. in a particular language which doesn't affect our analytical process. So, we should remove these stop words from our corpus and focus on important words. \n",
    "\n",
    "NLTK provides a list of stopwords which we can use to remove stopwords from our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import stopwords\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_words = stopwords.words('english')\n",
    "#let's have a look at few of them\n",
    "stopwords.words('english')[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stopwords from a corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " '``',\n",
       " 'knowledge',\n",
       " 'engineer',\n",
       " \"''\",\n",
       " 'interviews',\n",
       " 'experts',\n",
       " 'certain',\n",
       " 'domain',\n",
       " 'tries',\n",
       " 'embody',\n",
       " 'knowledge',\n",
       " 'computer',\n",
       " 'program',\n",
       " 'carrying',\n",
       " 'task',\n",
       " '.',\n",
       " 'How',\n",
       " 'well',\n",
       " 'works',\n",
       " 'depends',\n",
       " 'whether',\n",
       " 'intellectual',\n",
       " 'mechanisms',\n",
       " 'required',\n",
       " 'task',\n",
       " 'within',\n",
       " 'present',\n",
       " 'state',\n",
       " 'AI',\n",
       " '.',\n",
       " 'When',\n",
       " 'turned',\n",
       " ',',\n",
       " 'many',\n",
       " 'disappointing',\n",
       " 'results',\n",
       " '.',\n",
       " 'One',\n",
       " 'first',\n",
       " 'expert',\n",
       " 'systems',\n",
       " 'MYCIN',\n",
       " '1974',\n",
       " ',',\n",
       " 'diagnosed',\n",
       " 'bacterial',\n",
       " 'infections',\n",
       " 'blood',\n",
       " 'suggested',\n",
       " 'treatments',\n",
       " '.',\n",
       " 'It',\n",
       " 'better',\n",
       " 'medical',\n",
       " 'students',\n",
       " 'practicing',\n",
       " 'doctors',\n",
       " ',',\n",
       " 'provided',\n",
       " 'limitations',\n",
       " 'observed',\n",
       " '.',\n",
       " 'Namely',\n",
       " ',',\n",
       " 'ontology',\n",
       " 'included',\n",
       " 'bacteria',\n",
       " ',',\n",
       " 'symptoms',\n",
       " ',',\n",
       " 'treatments',\n",
       " 'include',\n",
       " 'patients',\n",
       " ',',\n",
       " 'doctors',\n",
       " ',',\n",
       " 'hospitals',\n",
       " ',',\n",
       " 'death',\n",
       " ',',\n",
       " 'recovery',\n",
       " ',',\n",
       " 'events',\n",
       " 'occurring',\n",
       " 'time',\n",
       " '.',\n",
       " 'Its',\n",
       " 'interactions',\n",
       " 'depended',\n",
       " 'single',\n",
       " 'patient',\n",
       " 'considered',\n",
       " '.',\n",
       " 'Since',\n",
       " 'experts',\n",
       " 'consulted',\n",
       " 'knowledge',\n",
       " 'engineers',\n",
       " 'knew',\n",
       " 'patients',\n",
       " ',',\n",
       " 'doctors',\n",
       " ',',\n",
       " 'death',\n",
       " ',',\n",
       " 'recovery',\n",
       " ',',\n",
       " 'etc.',\n",
       " ',',\n",
       " 'clear',\n",
       " 'knowledge',\n",
       " 'engineers',\n",
       " 'forced',\n",
       " 'experts',\n",
       " 'told',\n",
       " 'predetermined',\n",
       " 'framework',\n",
       " '.',\n",
       " 'In',\n",
       " 'present',\n",
       " 'state',\n",
       " 'AI',\n",
       " ',',\n",
       " 'true',\n",
       " '.',\n",
       " 'The',\n",
       " 'usefulness',\n",
       " 'current',\n",
       " 'expert',\n",
       " 'systems',\n",
       " 'depends',\n",
       " 'users',\n",
       " 'common',\n",
       " 'sense',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we will use word tokenized text for further processing.\n",
    "filtered_corpus = [w for w in words if not w in s_words]\n",
    "\n",
    "filtered_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure that our new filtered corpus is really filtered.\n",
    "Most of the corpus even if it's a one sentence contains few stopwords.\n",
    "Obviously, lenth of the filtered text should be less than old corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_corpus) < len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    }
   ],
   "source": [
    "#How much words did we remove?\n",
    "print(len(words) - len(filtered_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS Tagging (Part of Speech Taggin) is a process of classification in which we classify words into a lexical category such as noun, verb.\n",
    "Apparently, it's very useful for future analytical processes such as structure analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'DT'),\n",
       " ('``', '``'),\n",
       " ('knowledge', 'NN'),\n",
       " ('engineer', 'NN'),\n",
       " (\"''\", \"''\"),\n",
       " ('interviews', 'NNS'),\n",
       " ('experts', 'NNS'),\n",
       " ('certain', 'JJ'),\n",
       " ('domain', 'NN'),\n",
       " ('tries', 'NNS'),\n",
       " ('embody', 'VBP'),\n",
       " ('knowledge', 'JJ'),\n",
       " ('computer', 'NN'),\n",
       " ('program', 'NN'),\n",
       " ('carrying', 'NN'),\n",
       " ('task', 'NN'),\n",
       " ('.', '.'),\n",
       " ('How', 'WRB'),\n",
       " ('well', 'RB'),\n",
       " ('works', 'VBZ'),\n",
       " ('depends', 'VBZ'),\n",
       " ('whether', 'IN'),\n",
       " ('intellectual', 'JJ'),\n",
       " ('mechanisms', 'NNS'),\n",
       " ('required', 'VBN'),\n",
       " ('task', 'NN'),\n",
       " ('within', 'IN'),\n",
       " ('present', 'JJ'),\n",
       " ('state', 'NN'),\n",
       " ('AI', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('When', 'WRB'),\n",
       " ('turned', 'VBD'),\n",
       " (',', ','),\n",
       " ('many', 'JJ'),\n",
       " ('disappointing', 'JJ'),\n",
       " ('results', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('One', 'CD'),\n",
       " ('first', 'JJ'),\n",
       " ('expert', 'JJ'),\n",
       " ('systems', 'NNS'),\n",
       " ('MYCIN', 'NNP'),\n",
       " ('1974', 'CD'),\n",
       " (',', ','),\n",
       " ('diagnosed', 'VBD'),\n",
       " ('bacterial', 'JJ'),\n",
       " ('infections', 'NNS'),\n",
       " ('blood', 'NN'),\n",
       " ('suggested', 'VBD'),\n",
       " ('treatments', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('It', 'PRP'),\n",
       " ('better', 'RBR'),\n",
       " ('medical', 'JJ'),\n",
       " ('students', 'NNS'),\n",
       " ('practicing', 'VBG'),\n",
       " ('doctors', 'NNS'),\n",
       " (',', ','),\n",
       " ('provided', 'VBN'),\n",
       " ('limitations', 'NNS'),\n",
       " ('observed', 'VBN'),\n",
       " ('.', '.'),\n",
       " ('Namely', 'RB'),\n",
       " (',', ','),\n",
       " ('ontology', 'NN'),\n",
       " ('included', 'VBD'),\n",
       " ('bacteria', 'NNS'),\n",
       " (',', ','),\n",
       " ('symptoms', 'NNS'),\n",
       " (',', ','),\n",
       " ('treatments', 'NNS'),\n",
       " ('include', 'VBP'),\n",
       " ('patients', 'NNS'),\n",
       " (',', ','),\n",
       " ('doctors', 'NNS'),\n",
       " (',', ','),\n",
       " ('hospitals', 'NNS'),\n",
       " (',', ','),\n",
       " ('death', 'NN'),\n",
       " (',', ','),\n",
       " ('recovery', 'NN'),\n",
       " (',', ','),\n",
       " ('events', 'NNS'),\n",
       " ('occurring', 'VBG'),\n",
       " ('time', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Its', 'PRP$'),\n",
       " ('interactions', 'NNS'),\n",
       " ('depended', 'VBN'),\n",
       " ('single', 'JJ'),\n",
       " ('patient', 'NN'),\n",
       " ('considered', 'VBN'),\n",
       " ('.', '.'),\n",
       " ('Since', 'IN'),\n",
       " ('experts', 'NNS'),\n",
       " ('consulted', 'VBN'),\n",
       " ('knowledge', 'NN'),\n",
       " ('engineers', 'NNS'),\n",
       " ('knew', 'VBD'),\n",
       " ('patients', 'NNS'),\n",
       " (',', ','),\n",
       " ('doctors', 'NNS'),\n",
       " (',', ','),\n",
       " ('death', 'NN'),\n",
       " (',', ','),\n",
       " ('recovery', 'NN'),\n",
       " (',', ','),\n",
       " ('etc.', 'FW'),\n",
       " (',', ','),\n",
       " ('clear', 'JJ'),\n",
       " ('knowledge', 'NN'),\n",
       " ('engineers', 'NNS'),\n",
       " ('forced', 'VBD'),\n",
       " ('experts', 'NNS'),\n",
       " ('told', 'VBD'),\n",
       " ('predetermined', 'VBN'),\n",
       " ('framework', 'NN'),\n",
       " ('.', '.'),\n",
       " ('In', 'IN'),\n",
       " ('present', 'JJ'),\n",
       " ('state', 'NN'),\n",
       " ('AI', 'NNP'),\n",
       " (',', ','),\n",
       " ('true', 'JJ'),\n",
       " ('.', '.'),\n",
       " ('The', 'DT'),\n",
       " ('usefulness', 'JJ'),\n",
       " ('current', 'JJ'),\n",
       " ('expert', 'NN'),\n",
       " ('systems', 'NNS'),\n",
       " ('depends', 'VBZ'),\n",
       " ('users', 'JJ'),\n",
       " ('common', 'JJ'),\n",
       " ('sense', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here, we are going to use pos tagger provided by nltk, we can train our own pos_tag classifier as well\n",
    "#import\n",
    "from nltk import pos_tag\n",
    "\n",
    "#assign tags\n",
    "tags = pos_tag(filtered_corpus)\n",
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at  - <a href=\"http://www.winwaed.com/blog/2011/11/08/part-of-speech-tags/\"> List of POS Tags </a> . To know more about these words (NN, RB, VBZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunking is basically identification of parts of speech and short phrases ( i.e noun phrase ) in a text. Parts of Speech gives us information about whether words are noun, adjective or verbs, but sometimes it is useful to have more information about the structure of a text. We use chunking in <a href=\"http://en.wikipedia.org/wiki/Named_entity_recognition\"> Named Entity Recognition </a> in which we are interested in finding named entity in the text.\n",
    "\n",
    "Chunking is useful for extracting meaningful information from a text. Chunking allows us to extract group of words with set characteristics. \n",
    "\n",
    "For ex - john was driving so fast.\n",
    "\n",
    "In the above example, named entity for john will be \"Person\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.chunk import RegexpParser\n",
    "from nltk import sent_tokenize,word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pattern = \"\"\"\n",
    "    NP: {<JJ>*<NN>+}\n",
    "    {<JJ>*<NN><CC>*<NN>+}\n",
    "    \"\"\"\n",
    "#pattern to detect noun phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chunker = RegexpParser(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "he National Wrestling Association was an early professional wrestling sanctioning body created in 1930 by \n",
    "the National Boxing Association (NBA) (now the World Boxing Association, WBA) as an attempt to create\n",
    "a governing body for professional wrestling in the United States. The group created a number of \"World\" level \n",
    "championships as an attempt to clear up the professional wrestling rankings which at the time saw a number of \n",
    "different championships promoted as the \"true world championship\". The National Wrestling Association's NWA \n",
    "World Heavyweight Championship was later considered part of the historical lineage of the National Wrestling \n",
    "Alliance's NWA World Heavyweight Championship when then National Wrestling Association champion Lou Thesz \n",
    "won the National Wrestling Alliance championship, folding the original championship into one title in 1949.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_sentence = nltk.sent_tokenize(text)  # Tokenize the text into sentences.\n",
    "tokenized_words = [nltk.word_tokenize(sentence) for sentence in tokenized_sentence]  # Tokenize words in sentences.\n",
    "tagged_words = [nltk.pos_tag(word) for word in tokenized_words]  # Tag words for POS in each sentence.\n",
    "word_tree = [chunker.parse(word) for word in tagged_words]  # Identify NP chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_tree[0].draw() #draw the tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named Entity Recognition is one of the most important task in information extraction. It basically means extracting named entities in a text. For example, if we have a text - \"John Studies at Stanford\" then NER for john will be Person, for Stanford it will be Organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON John/NNP)\n",
      "  studies/NNS\n",
      "  at/IN\n",
      "  (ORGANIZATION Stanford/NNP University/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk, pos_tag,  word_tokenize\n",
    " \n",
    "sentence = \"John studies at Stanford University.\"\n",
    " \n",
    "print(ne_chunk(pos_tag(word_tokenize(sentence))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatizing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and Lemmatizing both really means removing morphological affixes from words, that is, leaving only the word stem. This is very helpful in many natural language processing tasks. Stem of a word \"running\" is \"run\", \"makes\" -> \"make\",etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmer.stem(\"running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'make'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem(\"makes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#another stemmer is snowball\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer2 = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'grow'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer2.stem(\"grows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is closely related to stemming. But lemmatization also takes into account the context in which word appears. Stemmers are just a set of rules, such as remove \"s\" from the end if xyz condition is satisfied. Stemmers are faster than lemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'make'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"makes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can also use <a href=\"https://stanfordnlp.github.io/CoreNLP/\">stanford core nlp modules</a> such as NER tagger in nltk. Usually they are faster and more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word sense is one of the meaning of a word. There are some english words which has multiple meanings such as bench. NLTK provides an api for WordNet which is a semantic graph for words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('man.n.01'),\n",
       " Synset('serviceman.n.01'),\n",
       " Synset('man.n.03'),\n",
       " Synset('homo.n.02'),\n",
       " Synset('man.n.05'),\n",
       " Synset('man.n.06'),\n",
       " Synset('valet.n.01'),\n",
       " Synset('man.n.08'),\n",
       " Synset('man.n.09'),\n",
       " Synset('man.n.10'),\n",
       " Synset('world.n.08'),\n",
       " Synset('man.v.01'),\n",
       " Synset('man.v.02')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "wn.synsets('man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'someone who serves in the armed forces; a member of a military force'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('man')[1].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('dog.n.01'),\n",
       " Synset('frump.n.01'),\n",
       " Synset('dog.n.03'),\n",
       " Synset('cad.n.01'),\n",
       " Synset('frank.n.02'),\n",
       " Synset('pawl.n.01'),\n",
       " Synset('andiron.n.01'),\n",
       " Synset('chase.v.01')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dog = wn.synset('dog.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the dog barked all night'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog.examples()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('canine.n.02'), Synset('domestic_animal.n.01')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog.hypernyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is usually required to count the frequency of a word in given text. We can use FreqDist to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = \"\"\"Science is the methodical study of nature including testable explanations and predictions. From classical antiquity through the 19th century, science as a type of knowledge was more closely linked to philosophy than it is now and, in fact, in the Western world, the term \"natural philosophy\" encompassed fields of study that are today associated with science, such as astronomy, medicine, and physics. However, during the Islamic Golden Age foundations for the scientific method were laid by Ibn al-Haytham in his Book of Optics. While the classification of the material world by the ancient Indians and Greeks into air, earth, fire and water was more philosophical, medieval Middle Easterns used practical, experimental observation to classify materials.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fd = FreqDist(nltk.word_tokenize(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({\"''\": 1,\n",
       "          ',': 12,\n",
       "          '.': 4,\n",
       "          '19th': 1,\n",
       "          'Age': 1,\n",
       "          'Book': 1,\n",
       "          'Easterns': 1,\n",
       "          'From': 1,\n",
       "          'Golden': 1,\n",
       "          'Greeks': 1,\n",
       "          'However': 1,\n",
       "          'Ibn': 1,\n",
       "          'Indians': 1,\n",
       "          'Islamic': 1,\n",
       "          'Middle': 1,\n",
       "          'Optics': 1,\n",
       "          'Science': 1,\n",
       "          'Western': 1,\n",
       "          'While': 1,\n",
       "          '``': 1,\n",
       "          'a': 1,\n",
       "          'air': 1,\n",
       "          'al-Haytham': 1,\n",
       "          'ancient': 1,\n",
       "          'and': 5,\n",
       "          'antiquity': 1,\n",
       "          'are': 1,\n",
       "          'as': 2,\n",
       "          'associated': 1,\n",
       "          'astronomy': 1,\n",
       "          'by': 2,\n",
       "          'century': 1,\n",
       "          'classical': 1,\n",
       "          'classification': 1,\n",
       "          'classify': 1,\n",
       "          'closely': 1,\n",
       "          'during': 1,\n",
       "          'earth': 1,\n",
       "          'encompassed': 1,\n",
       "          'experimental': 1,\n",
       "          'explanations': 1,\n",
       "          'fact': 1,\n",
       "          'fields': 1,\n",
       "          'fire': 1,\n",
       "          'for': 1,\n",
       "          'foundations': 1,\n",
       "          'his': 1,\n",
       "          'in': 3,\n",
       "          'including': 1,\n",
       "          'into': 1,\n",
       "          'is': 2,\n",
       "          'it': 1,\n",
       "          'knowledge': 1,\n",
       "          'laid': 1,\n",
       "          'linked': 1,\n",
       "          'material': 1,\n",
       "          'materials': 1,\n",
       "          'medicine': 1,\n",
       "          'medieval': 1,\n",
       "          'method': 1,\n",
       "          'methodical': 1,\n",
       "          'more': 2,\n",
       "          'natural': 1,\n",
       "          'nature': 1,\n",
       "          'now': 1,\n",
       "          'observation': 1,\n",
       "          'of': 5,\n",
       "          'philosophical': 1,\n",
       "          'philosophy': 2,\n",
       "          'physics': 1,\n",
       "          'practical': 1,\n",
       "          'predictions': 1,\n",
       "          'science': 2,\n",
       "          'scientific': 1,\n",
       "          'study': 2,\n",
       "          'such': 1,\n",
       "          'term': 1,\n",
       "          'testable': 1,\n",
       "          'than': 1,\n",
       "          'that': 1,\n",
       "          'the': 9,\n",
       "          'through': 1,\n",
       "          'to': 2,\n",
       "          'today': 1,\n",
       "          'type': 1,\n",
       "          'used': 1,\n",
       "          'was': 2,\n",
       "          'water': 1,\n",
       "          'were': 1,\n",
       "          'with': 1,\n",
       "          'world': 2})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification Example - Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Check out <a href=\"https://github.com/savan77/Practical-Machine-Learning-With-Python/blob/master/Part%20-%202/Sentiment%20Analysis.ipynb\">this notebook </a>for sentiment analysis task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Detection Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out <a href=\"Emotion%20Detection.ipynb\">notebook</a> on Emotion Detection task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
